import torch
import subprocess

if torch.cuda.is_available():
    print("CUDAå¯ç”¨")
    device_count = torch.cuda.device_count()
    print(f"ğŸ“Š å¯ç”¨GPUæ•°é‡: {device_count}")

    for i in range(device_count):
        print(f"\n--- GPU {i} è¯¦ç»†ä¿¡æ¯ ---")
        # è·å–è®¾å¤‡å±æ€§
        props = torch.cuda.get_device_properties(i)
        print(f"  è®¾å¤‡åç§°: {props.name}")
        print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        print(f"  æ€»æ˜¾å­˜: {props.total_memory / (1024 ** 3):.2f} GB")  # è½¬æ¢ä¸ºGB
        print(f"  å¤šå¤„ç†å™¨æ•°é‡: {props.multi_processor_count}")

        # è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ (å¯é€‰ï¼Œéœ€è¦é¢å¤–çš„nvidia-smiè°ƒç”¨)
        try:
            result = subprocess.check_output([
                'nvidia-smi', '--query-gpu=memory.used',
                '--format=csv,noheader,nounits', '-i', str(i)
            ], encoding='utf-8')
            memory_used = int(result.strip())
            print(f"  å·²ç”¨æ˜¾å­˜: {memory_used} MB")
        except Exception as e:
            print(f"  æ— æ³•è·å–å·²ç”¨æ˜¾å­˜: {e}")

else:
    print("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")

print("æ£€æŸ¥tree-sitterå…¼å®¹...")
from tree_sitter_languages import get_language, get_parser
parser = get_parser('cpp')
language = get_language('cpp')
if parser is None or language is None:
    print("tree-sitterå…¼å®¹å¤±è´¥")
else:
    print("tree-sitterå…¼å®¹æ£€æŸ¥æˆåŠŸ")